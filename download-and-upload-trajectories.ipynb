{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "points = gpd.read_file(\n",
    "    \"points.gpkg\", layer=\"kappazunder_image_punkte__ogdwienkappazunderimagepogd\"\n",
    ")\n",
    "points = points.set_index(\"image_name\")\n",
    "\n",
    "trajectories = gpd.read_file(\"trajectories.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"download-state.db\")\n",
    "conn.row_factory = sqlite3.Row\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS trajectories (\n",
    "    trajectory_id TEXT PRIMARY KEY,\n",
    "\n",
    "    download_id TEXT,\n",
    "    download_bytes INTEGER,\n",
    "    download_expires_at TEXT,\n",
    "\n",
    "    is_sensor1_completed INTEGER DEFAULT 0,\n",
    "    is_sensor2_completed INTEGER DEFAULT 0,\n",
    "    is_sensor3_completed INTEGER DEFAULT 0,\n",
    "    is_sensor4_completed INTEGER DEFAULT 0,\n",
    "\n",
    "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "if cur.execute(\"SELECT 1 FROM trajectories LIMIT 1;\").fetchone() is None:\n",
    "    for row in trajectories.itertuples():\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR IGNORE INTO trajectories (trajectory_id)\n",
    "            VALUES (?);\n",
    "        \"\"\",\n",
    "            (row.trajectoryid,),\n",
    "        )\n",
    "    print(\"No download-state.db found, initialized with trajectory IDs.\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_state_df = pd.read_sql_query(\"SELECT * FROM trajectories;\", conn)\n",
    "\n",
    "if \"download_expires_at\" in download_state_df.columns:\n",
    "    download_state_df[\"download_expires_at\"] = pd.to_datetime(\n",
    "        download_state_df[\"download_expires_at\"]\n",
    "    )\n",
    "\n",
    "trajectories = trajectories.merge(\n",
    "    download_state_df, how=\"left\", left_on=\"trajectoryid\", right_on=\"trajectory_id\"\n",
    ")\n",
    "\n",
    "trajectories = trajectories.drop(columns=[\"trajectory_id\"])\n",
    "\n",
    "for col in [\"download_id\", \"download_bytes\", \"download_expires_at\"]:\n",
    "    if col in trajectories.columns:\n",
    "        trajectories[col] = trajectories[col].where(trajectories[col].notna(), None)\n",
    "\n",
    "for col in [f\"is_sensor{i}_completed\" for i in range(1, 5)]:\n",
    "    if col in trajectories.columns:\n",
    "        trajectories[col] = trajectories[col].fillna(0).astype(int)\n",
    "\n",
    "print(\"SQLite state merged into trajectories DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_download_state(trajectory_id, **fields):\n",
    "    if not fields:\n",
    "        return\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    columns = \", \".join(f\"{col} = ?\" for col in fields.keys())\n",
    "    params = list(fields.values()) + [trajectory_id]\n",
    "\n",
    "    query = f\"\"\"\n",
    "        UPDATE trajectories\n",
    "        SET {columns},\n",
    "            updated_at = CURRENT_TIMESTAMP\n",
    "        WHERE trajectory_id = ?;\n",
    "    \"\"\"\n",
    "\n",
    "    cur.execute(query, params)\n",
    "    conn.commit()\n",
    "\n",
    "    idx = trajectories.index[trajectories[\"trajectoryid\"] == trajectory_id]\n",
    "\n",
    "    if len(idx) == 0:\n",
    "        print(f\"Warning: trajectory_id {trajectory_id} not found in DataFrame.\")\n",
    "        return\n",
    "\n",
    "    idx = idx[0]\n",
    "\n",
    "    for col, val in fields.items():\n",
    "        if col == \"download_expires_at\" and isinstance(val, str):\n",
    "            val = pd.to_datetime(val)\n",
    "\n",
    "        trajectories.at[idx, col] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxBZ8wjAPQz1"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import piexif\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def get_download_id_and_size_in_bytes(trajectory_id):\n",
    "    polygon = trajectories[\n",
    "        trajectories[\"trajectoryid\"] == trajectory_id\n",
    "    ].geometry.values[0]\n",
    "    coords = [[x, y] for x, y in polygon.exterior.coords]\n",
    "    url = \"https://mein.wien.gv.at/geodownload-backend/app/register\"\n",
    "    payload = {\"data\": {\"coords\": coords, \"dataset\": \"KAPPAZUNDER 2020\", \"option\": 2}}\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    items = response.json().get(\"items\")\n",
    "    return items.get(\"confirmation\"), items.get(\"size\") * 1024 * 1024\n",
    "\n",
    "\n",
    "def request_confirm_email(download_id, email_address):\n",
    "    with requests.Session() as session:\n",
    "        session.get(\n",
    "            f\"https://mein.wien.gv.at/geodownload-ui/confirm/{download_id}\",\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            },\n",
    "        )\n",
    "        response = session.patch(\n",
    "            f\"https://mein.wien.gv.at/geodownload-backend/app/confirm/{download_id}\",\n",
    "            json={\"mail\": email_address},\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"application/json, text/plain, */*\",\n",
    "                \"Origin\": \"https://mein.wien.gv.at\",\n",
    "                \"Referer\": \"https://mein.wien.gv.at/\",\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "def confirm_email():\n",
    "    url = \"https://mein.wien.gv.at/geodownload-backend/app/mail/2d4bf8b8-88cb-4c9c-b29b-bf2f5ef50c8f\"\n",
    "    response = requests.patch(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "\n",
    "def download(download_id, size):\n",
    "    url = f\"https://www.wien.gv.at/ogdgeodata/download/{download_id}.tar\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    with requests.get(url, headers=headers, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        block_size = 8 * 1024\n",
    "        progress_bar = tqdm(\n",
    "            total=size,\n",
    "            unit=\"iB\",\n",
    "            unit_scale=True,\n",
    "            desc=f\"Downloading {download_id}.tar\",\n",
    "        )\n",
    "        with open(download_id + \".tar\", \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=block_size):\n",
    "                progress_bar.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        progress_bar.close()\n",
    "\n",
    "\n",
    "def extract_and_remove_tar(download_id):\n",
    "    tar_path = download_id + \".tar\"\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        tar.extractall(path=download_id)\n",
    "    os.remove(tar_path)\n",
    "\n",
    "\n",
    "def get_trajectory_dir_paths(download_id, trajectory_id):\n",
    "    los_dirs = [f for f in os.listdir(download_id)]\n",
    "    possible_trajectory_dir_paths = [\n",
    "        os.path.join(\n",
    "            download_id, los_dir, \"Bild-Rohdaten\", f\"Trajektorie_{trajectory_id}\"\n",
    "        )\n",
    "        for los_dir in los_dirs\n",
    "    ]\n",
    "    return [d for d in possible_trajectory_dir_paths if os.path.isdir(d)]\n",
    "\n",
    "\n",
    "def prune_downloaded_data(download_id, trajectory_dir_paths_to_keep):\n",
    "    for los_dir in os.listdir(download_id):\n",
    "        bild_rohdaten_dir_path = os.path.join(download_id, los_dir, \"Bild-Rohdaten\")\n",
    "        for trajectory_dir in os.listdir(bild_rohdaten_dir_path):\n",
    "            trajectory_dir_path = os.path.join(bild_rohdaten_dir_path, trajectory_dir)\n",
    "            if (\n",
    "                os.path.isdir(trajectory_dir_path)\n",
    "                and trajectory_dir_path not in trajectory_dir_paths_to_keep\n",
    "            ):\n",
    "                shutil.rmtree(trajectory_dir_path)\n",
    "\n",
    "\n",
    "def remove_top_and_bottom_facing_images(trajectory_dir_paths):\n",
    "    for trajectory_dir_path in trajectory_dir_paths:\n",
    "        for name in os.listdir(trajectory_dir_path):\n",
    "            if name.endswith((\"0\", \"5\")):\n",
    "                shutil.rmtree(os.path.join(trajectory_dir_path, name))\n",
    "\n",
    "\n",
    "def set_exif_tags(trajectory_dir_paths):\n",
    "    def deg_to_dms_rational(deg_float):\n",
    "        \"\"\"\n",
    "        Convert decimal degrees to EXIF DMS rational format.\n",
    "        \"\"\"\n",
    "        deg_abs = abs(deg_float)\n",
    "        deg = int(deg_abs)\n",
    "        min_float = (deg_abs - deg) * 60\n",
    "        min_ = int(min_float)\n",
    "        sec = round((min_float - min_) * 60 * 10000)\n",
    "        return ((deg, 1), (min_, 1), (sec, 10000))\n",
    "\n",
    "    file_paths = []\n",
    "    for trajectory_dir_path in trajectory_dir_paths:\n",
    "        for sensor_dir in os.listdir(trajectory_dir_path):\n",
    "            sensor_dir_path = os.path.join(trajectory_dir_path, sensor_dir)\n",
    "            sensor_dir_content_paths = [\n",
    "                os.path.join(sensor_dir_path, file)\n",
    "                for file in os.listdir(sensor_dir_path)\n",
    "            ]\n",
    "            file_paths.extend(\n",
    "                [file for file in sensor_dir_content_paths if os.path.isfile(file)]\n",
    "            )\n",
    "\n",
    "    for img_path in tqdm(file_paths, desc=\"Tagging images\"):\n",
    "        file = os.path.basename(img_path)\n",
    "        row = points.loc[file]\n",
    "        if row.empty:\n",
    "            print(f\"⚠️ No metadata found for {file}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Load existing EXIF or create new\n",
    "        exif_dict = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "\n",
    "        # GPS\n",
    "        exif_dict[\"GPS\"] = {\n",
    "            piexif.GPSIFD.GPSLatitudeRef: b\"N\",\n",
    "            piexif.GPSIFD.GPSLatitude: deg_to_dms_rational(row[\"lat\"]),\n",
    "            piexif.GPSIFD.GPSLongitudeRef: b\"E\",\n",
    "            piexif.GPSIFD.GPSLongitude: deg_to_dms_rational(row[\"lon\"]),\n",
    "        }\n",
    "\n",
    "        # DateTimeOriginal\n",
    "        exif_dict[\"Exif\"][piexif.ExifIFD.DateTimeOriginal] = (\n",
    "            row[\"epoch\"].strftime(\"%Y:%m:%d %H:%M:%S\").encode(\"utf-8\")\n",
    "        )\n",
    "\n",
    "        # Insert EXIF back into image\n",
    "        exif_bytes = piexif.dump(exif_dict)\n",
    "        piexif.insert(exif_bytes, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFeWJUJDPTTE"
   },
   "outputs": [],
   "source": [
    "import imaplib\n",
    "import email\n",
    "from email.header import decode_header\n",
    "import re\n",
    "\n",
    "\n",
    "class ReadyToDownloadChecker:\n",
    "    # Exact URL pattern with GUID\n",
    "    GUID_REGEX = re.compile(\n",
    "        r\"https://www\\.wien\\.gv\\.at/ogdgeodata/download/\"\n",
    "        r\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\\.tar\"\n",
    "    )\n",
    "\n",
    "    def __init__(self, email_address, app_password, email_subject_filter):\n",
    "        self.email_address = email_address\n",
    "        self.app_password = app_password\n",
    "        self.subject_filter = email_subject_filter\n",
    "        self.imap = None\n",
    "        self.emails = []  # List of (subject, body)\n",
    "        self._connect()\n",
    "        self._fetch_emails_by_subject()\n",
    "\n",
    "    def _connect(self):\n",
    "        if self.imap:\n",
    "            try:\n",
    "                self.imap.logout()\n",
    "            except:\n",
    "                pass\n",
    "        self.imap = imaplib.IMAP4_SSL(\"imap.gmail.com\")\n",
    "        self.imap.login(self.email_address, self.app_password)\n",
    "        self.imap.select(\"inbox\")\n",
    "\n",
    "    def _fetch_emails_by_subject(self):\n",
    "        search_criterion = f'(SUBJECT \"{self.subject_filter}\")'\n",
    "        status, messages = self.imap.search(None, search_criterion)\n",
    "        if status != \"OK\":\n",
    "            return\n",
    "        for num in messages[0].split():\n",
    "            self._fetch_email(num)\n",
    "\n",
    "    def _fetch_email(self, num):\n",
    "        status, msg_data = self.imap.fetch(num, \"(RFC822)\")\n",
    "        if status != \"OK\":\n",
    "            return\n",
    "        msg = email.message_from_bytes(msg_data[0][1])\n",
    "        subject = decode_header(msg.get(\"Subject\") or \"\")[0][0]\n",
    "        if isinstance(subject, bytes):\n",
    "            subject = subject.decode(errors=\"ignore\")\n",
    "        body = \"\"\n",
    "        if msg.is_multipart():\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == \"text/plain\":\n",
    "                    try:\n",
    "                        body += part.get_payload(decode=True).decode(errors=\"ignore\")\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            try:\n",
    "                body = msg.get_payload(decode=True).decode(errors=\"ignore\")\n",
    "            except:\n",
    "                pass\n",
    "        self.emails.append((subject, body))\n",
    "\n",
    "    def refresh(self):\n",
    "        \"\"\"Fetch only unseen emails with matching subject\"\"\"\n",
    "        try:\n",
    "            self._connect()\n",
    "            search_criterion = f'(UNSEEN SUBJECT \"{self.subject_filter}\")'\n",
    "            status, messages = self.imap.search(None, search_criterion)\n",
    "            if status == \"OK\":\n",
    "                for num in messages[0].split():\n",
    "                    self._fetch_email(num)\n",
    "        except Exception as e:\n",
    "            print(\"Error refreshing mail:\", e)\n",
    "\n",
    "    def get_ids(self):\n",
    "        \"\"\"Return a deduplicated list of GUIDs from cached emails\"\"\"\n",
    "        guids = set()\n",
    "        for _, body in self.emails:\n",
    "            matches = self.GUID_REGEX.findall(body)\n",
    "            guids.update(matches)\n",
    "        return list(guids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 903266,
     "status": "error",
     "timestamp": 1762724174120,
     "user": {
      "displayName": "Elias Gander",
      "userId": "00320090336903503101"
     },
     "user_tz": -60
    },
    "id": "DuxL3k3-PU79",
    "outputId": "e8e4310b-638c-4b0b-8866-5c96fc911559"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "\n",
    "MAPILLARY_USER = \"eliasgander\"\n",
    "MAPILLARY_EMAIL = \"daring.64tum@icloud.com\"\n",
    "MAPILLARY_PASSWORD = \"cesjuD-6tyjmo-maqgif\"\n",
    "EMAIL_ADDRESS = \"tomturbo657@gmail.com\"\n",
    "GMAIL_APP_PASSWORD = \"kruf aahg aiuc iwtr\"\n",
    "DOWNLOAD_READY_SUBJECT = \"Download-Link zu Ihren Geodaten\"\n",
    "VOLUME_SIZE_IN_BYTES = 40 * 1024 * 1024 * 1024  # 40GB\n",
    "IS_DEBUG = False\n",
    "\n",
    "if IS_DEBUG:\n",
    "  update_download_state('17720', download_id='5276d431-a054-4a84-a38c-6dfbccefdef0', download_bytes=1403*1024*1024, download_expires_at=(pd.Timestamp.now() + timedelta(days=7)).isoformat())\n",
    "  update_download_state('16101', download_id='56a27033-35ed-4c3c-ba14-704cc256efac', download_bytes=668*1024*1024, download_expires_at=(pd.Timestamp.now() + timedelta(days=7)).isoformat())\n",
    "  update_download_state('16471', download_id='e4a3d58b-2c58-4991-8e98-ae7b635d25cf', download_bytes=720*1024*1024, download_expires_at=(pd.Timestamp.now() + timedelta(days=7)).isoformat())\n",
    "\n",
    "!mapillary_tools authenticate --user_name {MAPILLARY_USER} --user_email {MAPILLARY_EMAIL} --user_password {MAPILLARY_PASSWORD}\n",
    "\n",
    "ready_to_download = ReadyToDownloadChecker(EMAIL_ADDRESS, GMAIL_APP_PASSWORD, DOWNLOAD_READY_SUBJECT)\n",
    "\n",
    "sensors_completed_column_names = [f\"is_sensor{i}_completed\" for i in range(1, 5)]\n",
    "\n",
    "while not trajectories[sensors_completed_column_names].to_numpy().all():\n",
    "    print(f\"Number of uncompleted trajectories: {trajectories[~trajectories[sensors_completed_column_names].to_numpy().all(axis=1)].shape[0]}\")\n",
    "\n",
    "    expiring_trajectories = trajectories[trajectories['download_expires_at'].notna() & (trajectories['download_expires_at'] < pd.Timestamp.now() + timedelta(hours=5))]\n",
    "    if not expiring_trajectories.empty:\n",
    "        print(f\"Resetting expiring trajectories with IDs: {expiring_trajectories['trajectoryid'].tolist()}\")\n",
    "        for trajectory_id in expiring_trajectories[\"trajectoryid\"]:\n",
    "          update_download_state(\n",
    "              trajectory_id,\n",
    "              download_id=None,\n",
    "              download_bytes=None,\n",
    "              download_expires_at=None\n",
    "          )\n",
    "\n",
    "    ready_to_download.refresh()\n",
    "    trajectories_to_download = trajectories[trajectories['download_id'].isin(ready_to_download.get_ids())]\n",
    "    if len(trajectories_to_download) <= 5:\n",
    "        print(f\"Only {len(trajectories_to_download)} downloadable trajectories left\")\n",
    "        trajectories_to_prepare = trajectories[trajectories['download_id'].isna()]\n",
    "        if trajectories_to_prepare.empty:\n",
    "            print(\"No trajectories left to prepare\")\n",
    "        else:\n",
    "            trajectories_to_prepare = trajectories_to_prepare.sample(min(10, len(trajectories)))\n",
    "            print(f\"Preparing {len(trajectories_to_prepare)} trajectories with ids: {trajectories_to_prepare['trajectoryid'].tolist()}\")\n",
    "            for index, trajectory in trajectories_to_prepare.iterrows():\n",
    "                trajectory_id = trajectory['trajectoryid']\n",
    "                try:\n",
    "                    download_id, size = get_download_id_and_size_in_bytes(trajectory_id)\n",
    "                    if size > VOLUME_SIZE_IN_BYTES / 2 * 0.95:\n",
    "                      print(f\"Skipping trajectoryid {trajectory_id} with downloadid {download_id} because size {size} exceeds limit.\")\n",
    "                      continue\n",
    "                    request_confirm_email(download_id, EMAIL_ADDRESS)\n",
    "                    sleep(60)\n",
    "                    confirm_email()\n",
    "                    update_download_state(trajectory_id, download_id=download_id, download_bytes=size, download_expires_at=(pd.Timestamp.now() + timedelta(days=7)).isoformat())\n",
    "                    print(f\"Successfully prepared trajectoryid {trajectory_id} with downloadid {download_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error preparing trajectoryid {trajectory_id} with downloadid {download_id}: {e}\")\n",
    "\n",
    "    if trajectories_to_download.empty:\n",
    "        print(\"No trajectories ready for download. Sleeping five minutes.\")\n",
    "        sleep(300)\n",
    "        continue\n",
    "\n",
    "    trajectory_to_download = trajectories_to_download.sort_values('download_expires_at').iloc[0]\n",
    "    trajectory_id = trajectory_to_download['trajectoryid']\n",
    "    download_id = trajectory_to_download['download_id']\n",
    "    if not os.path.isdir(download_id):\n",
    "      try:\n",
    "            download(download_id, trajectory_to_download['download_bytes'])\n",
    "      except HTTPError as e:\n",
    "          if e.code == 404:\n",
    "            print(f\"Resetting downloadid {download_id} of trajectory {trajectory_id} because 404 error: {e}\")\n",
    "            update_download_state(trajectory_id, download_id=None, download_bytes=None, download_expires_at=None)\n",
    "          else:\n",
    "            print(f\"Sleeping for five minutes because download of trajectory {trajectory_id} with downloadid {download_id} failed with message: {e}\")\n",
    "            sleep(300)\n",
    "          continue\n",
    "      extract_and_remove_tar(download_id)\n",
    "      \n",
    "    trajectory_dir_paths = get_trajectory_dir_paths(download_id, trajectory_id.split('_', 1)[0])\n",
    "    prune_downloaded_data(download_id, trajectory_dir_paths)\n",
    "    remove_top_and_bottom_facing_images(trajectory_dir_paths)\n",
    "    set_exif_tags(trajectory_dir_paths)\n",
    "    for i in range(1, 5):\n",
    "        if trajectories[trajectories['trajectoryid'] == trajectory_id][f'is_sensor{i}_completed'].iloc[0]:\n",
    "          continue\n",
    "        cmd = f\"\"\"\n",
    "        mapillary_tools process_and_upload \\\n",
    "            --overwrite_all_EXIF_tags \\\n",
    "            --device_make Teledyne \\\n",
    "            --device_model Ladybug6 \\\n",
    "            --offset_angle {(i - 1) * 90} \\\n",
    "            --interpolate_directions \\\n",
    "            --user_name {MAPILLARY_USER} \\\n",
    "            --noresume \\\n",
    "            {download_id}/*/Bild-Rohdaten/Trajektorie_{trajectory_id}/Sensor_*{i}/\n",
    "        \"\"\"\n",
    "        !{cmd}\n",
    "        update_download_state(trajectory_id, **{f'is_sensor{i}_completed': 1})\n",
    "    shutil.rmtree(download_id)\n",
    "\n",
    "\n",
    "print(\"All trajectories completed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOx2YobyJfj49KILurLjqIR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
